{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emha/anaconda3/envs/deeplearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('pdf')\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import importlib\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR, 'utils'))\n",
    "import tf_util\n",
    "#import visualization\n",
    "import provider\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GAUSSIANS = 5\n",
    "GMM_TYPE = 'grid'\n",
    "GMM_VARIANCE = 0.04\n",
    "\n",
    "NUM_CLASSES = 40\n",
    "BATCH_SIZE = 8\n",
    "NUM_POINT = 1024\n",
    "MAX_EPOCH = 200\n",
    "BASE_LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.9\n",
    "OPTIMIZER = 'adam'\n",
    "DECAY_STEP = 200000\n",
    "DECAY_RATE = 0.7\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "LIMIT_GPU = True\n",
    "\n",
    "MAX_ACCURACY = 0.0\n",
    "MAX_CLASS_ACCURACY = 0.0\n",
    "\n",
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n",
    "\n",
    "\n",
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        DECAY_STEP,          # Decay step.\n",
    "                        DECAY_RATE,          # Decay rate.\n",
    "                        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = importlib.import_module('3dmfv_net_c_cls') # import network module\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'models', '3dmfv_net_c_cls.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log dir already exists! creating a new one..............\n",
      "New log dir:log/modelnet40/3dmfv_net_c_cls/grid5_log_trial/8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creat log directory ant prevent over-write by creating numbered subdirectories\n",
    "log_dir = 'log_trial'\n",
    "LOG_DIR = 'log/modelnet' + str(NUM_CLASSES) + '/' + '3dmfv_net_c_cls' + '/'+ GMM_TYPE + str(N_GAUSSIANS) + '_' + log_dir\n",
    "augment_rotation, augment_scale, augment_translation, augment_jitter, augment_outlier = (False, True, True, True, False)\n",
    "\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "else:\n",
    "    print('Log dir already exists! creating a new one..............')\n",
    "    n = 0\n",
    "    while True:\n",
    "        n+=1\n",
    "        new_log_dir = LOG_DIR+'/'+str(n)\n",
    "        if not os.path.exists(new_log_dir):\n",
    "            os.makedirs(new_log_dir)\n",
    "            print('New log dir:'+new_log_dir)\n",
    "            break\n",
    "    log_dir = new_log_dir\n",
    "    LOG_DIR = new_log_dir\n",
    "\n",
    "\n",
    "os.system('cp %s %s' % (MODEL_FILE, LOG_DIR)) # bkp of model def\n",
    "os.system('cp train_cls.py %s' % (LOG_DIR)) # bkp of train procedure\n",
    "#pickle.dump(FLAGS, open( os.path.join(LOG_DIR, 'parameters.p'), \"wb\" ) )\n",
    "\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "#LOG_FOUT.write(str(FLAGS)+'\\n')\n",
    "LOG_FOUT.write(\"augmentation RSTJ = \" + str((augment_rotation, augment_scale, augment_translation, augment_jitter, augment_outlier))) #log augmentaitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Modelnet40\n"
     ]
    }
   ],
   "source": [
    "# ModelNet40 official train/test split. MOdelNet10 requires separate downloading and sampling.\n",
    "MAX_N_POINTS = 2048\n",
    "NUM_CLASSES = 40\n",
    "TRAIN_FILES = provider.getDataFiles( \\\n",
    "    os.path.join(BASE_DIR, 'data/modelnet'+str(NUM_CLASSES)+'_ply_hdf5_'+ str(MAX_N_POINTS)+ '/train_files.txt'))\n",
    "TEST_FILES = provider.getDataFiles(\\\n",
    "    os.path.join(BASE_DIR, 'data/modelnet'+str(NUM_CLASSES)+'_ply_hdf5_'+ str(MAX_N_POINTS)+ '/test_files.txt'))\n",
    "LABEL_MAP = provider.getDataFiles(\\\n",
    "    os.path.join(BASE_DIR, 'data/modelnet'+str(NUM_CLASSES)+'_ply_hdf5_'+ str(MAX_N_POINTS)+ '/shape_names.txt'))\n",
    "\n",
    "print( \"Loading Modelnet\" + str(NUM_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tf_util\n",
    "#import visualization\n",
    "#import provider\n",
    "import utils\n",
    "\n",
    "def train(gmm):\n",
    "    global MAX_ACCURACY, MAX_CLASS_ACCURACY\n",
    "    # n_fv_features = 7 * len(gmm.weights_)\n",
    "\n",
    "    # Build Graph, train and classify\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:0'):\n",
    "            points_pl, labels_pl, w_pl, mu_pl, sigma_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT, gmm )\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "            # Note the global_step=batch parameter to minimize.\n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "            # Get model and loss\n",
    "            pred, fv = MODEL.get_model(points_pl, w_pl, mu_pl, sigma_pl, is_training_pl, bn_decay=bn_decay, weigth_decay=WEIGHT_DECAY, add_noise=False, num_classes=NUM_CLASSES)\n",
    "            loss = MODEL.get_loss(pred, labels_pl)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            # Get accuracy\n",
    "            correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(labels_pl))\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            # Get training operator\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)#, aggregation_method = tf.AggregationMethod.EXPERIMENTAL_TREE) #consider using: tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N\n",
    "\n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        # Create a session\n",
    "        LIMIT_GPU = 1\n",
    "        sess = tf_util.get_session(0, limit_gpu=LIMIT_GPU)\n",
    "\n",
    "        # Add summary writers\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "\n",
    "        # Init variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'points_pl': points_pl,\n",
    "               'labels_pl': labels_pl,\n",
    "               'w_pl': w_pl,\n",
    "               'mu_pl': mu_pl,\n",
    "               'sigma_pl': sigma_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'fv': fv,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch\n",
    "              }\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "            log_string('**** EPOCH %03d ****' % (epoch))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            train_one_epoch(sess, ops, gmm, train_writer)\n",
    "            acc, acc_avg_cls = eval_one_epoch(sess, ops, gmm, test_writer)\n",
    "\n",
    "            # Save the variables to disk.\n",
    "            if epoch % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "            if acc > MAX_ACCURACY:\n",
    "                MAX_ACCURACY = acc\n",
    "                MAX_CLASS_ACCURACY = acc_avg_cls\n",
    "\n",
    "        log_string(\"Best test accuracy: %f\" % MAX_ACCURACY)\n",
    "        log_string(\"Best test class accuracy: %f\" % MAX_CLASS_ACCURACY)\n",
    "\n",
    "\n",
    "def train_one_epoch(sess, ops, gmm, train_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "\n",
    "    # Shuffle train files\n",
    "    train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "    np.random.shuffle(train_file_idxs)\n",
    "\n",
    "    for fn in range(len(TRAIN_FILES)):\n",
    "        log_string('----' + str(fn) + '-----')\n",
    "        current_data, current_label = provider.loadDataFile(TRAIN_FILES[train_file_idxs[fn]], compensate = False)\n",
    "        # points_idx = range(0,NUM_POINT)\n",
    "        points_idx = np.random.choice(range(0,2048),NUM_POINT)\n",
    "        current_data = current_data[:, points_idx, :]\n",
    "        current_data, current_label, _ = provider.shuffle_data(current_data, np.squeeze(current_label))\n",
    "        current_label = np.squeeze(current_label)\n",
    "\n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size / BATCH_SIZE\n",
    "\n",
    "        total_correct = 0\n",
    "        total_seen = 0\n",
    "        loss_sum = 0\n",
    "\n",
    "        for batch_idx in range(int(num_batches)):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx + 1) * BATCH_SIZE\n",
    "\n",
    "            # Augment batched point clouds by rotation and jittering\n",
    "\n",
    "            augmented_data = current_data[start_idx:end_idx, :, :]\n",
    "            if augment_scale:\n",
    "                augmented_data = provider.scale_point_cloud(augmented_data, smin=0.66, smax=1.5)\n",
    "            if augment_rotation:\n",
    "                augmented_data = provider.rotate_point_cloud(augmented_data)\n",
    "            if augment_translation:\n",
    "                augmented_data = provider.translate_point_cloud(augmented_data, tval = 0.2)\n",
    "            if augment_jitter:\n",
    "                augmented_data = provider.jitter_point_cloud(augmented_data, sigma=0.01,\n",
    "                                                        clip=0.05)  # default sigma=0.01, clip=0.05\n",
    "            if augment_outlier:\n",
    "                augmented_data = provider.insert_outliers_to_point_cloud(augmented_data, outlier_ratio=0.02)\n",
    "\n",
    "            feed_dict = {ops['points_pl']: augmented_data,\n",
    "                         ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                         ops['w_pl']: gmm.weights_,\n",
    "                         ops['mu_pl']: gmm.means_,\n",
    "                         ops['sigma_pl']: np.sqrt(gmm.covariances_),\n",
    "                         ops['is_training_pl']: is_training, }\n",
    "            summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                                                             ops['train_op'], ops['loss'], ops['pred']],\n",
    "                                                            feed_dict=feed_dict)\n",
    "\n",
    "            train_writer.add_summary(summary, step)\n",
    "            pred_val = np.argmax(pred_val, 1)\n",
    "            correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "            total_correct += correct\n",
    "            total_seen += BATCH_SIZE\n",
    "            loss_sum += loss_val\n",
    "\n",
    "        log_string('mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "        log_string('accuracy: %f' % (total_correct / float(total_seen)))\n",
    "\n",
    "\n",
    "def eval_one_epoch(sess, ops, gmm, test_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "\n",
    "    fail_cases_true_labels_final = []\n",
    "    fail_cases_false_labes_final = []\n",
    "    fail_cases_idx_final = []\n",
    "\n",
    "    # points_idx = np.random.choice(range(0, 2048), NUM_POINT)\n",
    "    points_idx = range(NUM_POINT)\n",
    "\n",
    "    for fn in range(len(TEST_FILES)):\n",
    "        log_string('----' + str(fn) + '-----')\n",
    "        current_data, current_label = provider.loadDataFile(TEST_FILES[fn], compensate=False)\n",
    "        current_data = current_data[:, points_idx, :]\n",
    "        current_label = np.squeeze(current_label)\n",
    "\n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size / BATCH_SIZE\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx + 1) * BATCH_SIZE\n",
    "\n",
    "            feed_dict = {ops['points_pl']: current_data[start_idx:end_idx, :, :] ,\n",
    "                         ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                         ops['w_pl']: gmm.weights_,\n",
    "                         ops['mu_pl']: gmm.means_,\n",
    "                         ops['sigma_pl']: np.sqrt(gmm.covariances_),\n",
    "                         ops['is_training_pl']: is_training}\n",
    "            summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                                                          ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "            test_writer.add_summary(summary, step)\n",
    "            pred_val = np.argmax(pred_val, 1)\n",
    "            correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "\n",
    "            #Find the fail cases\n",
    "            batch_current_label = current_label[start_idx:end_idx]\n",
    "            false_idx = pred_val != batch_current_label\n",
    "            fail_cases_true_labels = batch_current_label[np.where(false_idx)]  if batch_idx==0 else np.concatenate([fail_cases_true_labels,batch_current_label[np.where(false_idx)]] )\n",
    "            fail_cases_false_labes = pred_val[np.where(false_idx)]  if batch_idx==0 else np.concatenate([fail_cases_false_labes, pred_val[np.where(false_idx)]])\n",
    "            fail_cases_idx = false_idx if batch_idx == 0 else np.concatenate([fail_cases_idx, false_idx])\n",
    "\n",
    "            total_correct += correct\n",
    "            total_seen += BATCH_SIZE\n",
    "            loss_sum += (loss_val * BATCH_SIZE)\n",
    "            for i in range(start_idx, end_idx):\n",
    "                l = current_label[i]\n",
    "                total_seen_class[l] += 1\n",
    "                total_correct_class[l] += (pred_val[i - start_idx] == l)\n",
    "\n",
    "        fail_cases_true_labels_final.append(fail_cases_true_labels)\n",
    "        fail_cases_false_labes_final.append(fail_cases_false_labes)\n",
    "        fail_cases_idx_final.append(fail_cases_idx)\n",
    "    acc = total_correct / float(total_seen)\n",
    "    acc_avg_cls =  np.mean(np.array(total_correct_class) / np.array(total_seen_class, dtype=np.float))\n",
    "    log_string('eval mean loss: %f' % (loss_sum / float(total_seen)))\n",
    "    log_string('eval accuracy: %f' % (acc))\n",
    "    log_string('eval avg class acc: %f' % (acc_avg_cls))\n",
    "\n",
    "    FAIL_CASES_FOUT.write('True:' + str(fail_cases_true_labels) + '\\n')\n",
    "    FAIL_CASES_FOUT.write('Pred:' + str(fail_cases_false_labes) + '\\n')\n",
    "    FAIL_CASES_FOUT.write('Idx:' + str(fail_cases_idx) + '\\n')\n",
    "    FAIL_CASES_FOUT.flush()\n",
    "    dump_dic = {'true_labels': fail_cases_true_labels_final,\n",
    "                'false_pred_labels': fail_cases_false_labes_final,\n",
    "                'idxs': fail_cases_idx_final}\n",
    "    # pickle.dump([fail_cases_true_labels, fail_cases_false_labes], open(os.path.join(LOG_DIR, 'fail_cases.p'), \"wb\"))\n",
    "    pickle.dump(dump_dic, open(os.path.join(LOG_DIR, 'fail_cases.p'), \"wb\"))\n",
    "\n",
    "    return (acc, acc_avg_cls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/emha/machine_learning/3DmFV/3DmFV-Net/utils/tf_util.py:637: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "INFO:tensorflow:Summary name classify loss is illegal; using classify_loss instead.\n",
      "**** EPOCH 000 ****\n",
      "----0-----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0178f743723a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_3d_grid_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdivisions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN_GAUSSIANS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_GAUSSIANS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_GAUSSIANS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGMM_VARIANCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#pickle.dump(gmm, open(os.path.join(LOG_DIR, 'gmm.p'), \"wb\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#export_visualizations(gmm, LOG_DIR,n_model_limit=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-60feba723c77>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(gmm)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_avg_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-60feba723c77>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(sess, ops, gmm, train_writer)\u001b[0m\n\u001b[1;32m    139\u001b[0m             summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n\u001b[1;32m    140\u001b[0m                                                              ops['train_op'], ops['loss'], ops['pred']],\n\u001b[0;32m--> 141\u001b[0;31m                                                             feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gmm = utils.get_3d_grid_gmm(subdivisions=[N_GAUSSIANS, N_GAUSSIANS, N_GAUSSIANS], variance=GMM_VARIANCE)\n",
    "#pickle.dump(gmm, open(os.path.join(LOG_DIR, 'gmm.p'), \"wb\"))\n",
    "train(gmm)\n",
    "#export_visualizations(gmm, LOG_DIR,n_model_limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL2 = importlib.import_module('3dmfv_net_c_cls') # import network module\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'models', '3dmfv_net_c_cls.py')\n",
    "\n",
    "gmm = utils.get_3d_grid_gmm(subdivisions=[N_GAUSSIANS, N_GAUSSIANS, N_GAUSSIANS], variance=GMM_VARIANCE)\n",
    "po, labels_pl, w_pl, mu_pl, sigma_pl = MODEL2.placeholder_inputs(16, 1024, gmm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
